# LAST Training Configuration -- Default
# All hyperparameters for the training pipeline
#
# Batch size guidance (with AMP + adaptive graph):
#   Kaggle T4  (16GB): batch_size: 8,  accum: 4  → effective 32
#   GCP P100   (16GB): batch_size: 16, accum: 1  → effective 16
#   Lambda A10 (23GB, LAST-v2 base):   batch_size: 64  (3 streams × 9.2M params)
#   Lambda A10 (23GB, LAST-E any):     batch_size: 128 (tiny model, ample headroom)

training:
  # Optimizer (SGD + Nesterov is SOTA for GCNs like CTR-GCN)
  optimizer: "sgd"
  lr: 0.01         # Safe LR for batch=16 with adaptive graph (80% reduction from 0.05)
  momentum: 0.9    # Standard momentum
  nesterov: true   # Essential for convergence
  weight_decay: 0.0004  # Applied only to conv/linear weights (bias/BN/alpha/A_learned excluded)

  # LR Schedule: LinearLR warmup → CosineAnnealingLR via SequentialLR
  # Fully serialized into checkpoint — safe to resume at any epoch
  scheduler: "cosine_warmup"
  warmup_epochs: 10      # Extended warmup for adaptive graph stability
  warmup_start_lr: 0.001 # Start at lr/10
  min_lr: 0.0001         # Cosine floor

  # Training loop
  epochs: 70        # SOTA usually 65-80 epochs
  batch_size: 16    # Stable for P100 16GB with adaptive graph
  gradient_clip: 1.0  # Tighter clip for adaptive graph (A_learned can accumulate large grads)

  # Gradient accumulation
  # effective_batch = batch_size × gradient_accumulation_steps
  # GCP P100:  batch_size: 16, gradient_accumulation_steps: 1  → effective 16
  # Kaggle T4: batch_size: 8, gradient_accumulation_steps: 4  → effective 32
  gradient_accumulation_steps: 1

  # Loss
  label_smoothing: 0.1

  # Input
  input_frames: 64  # temporal crop during training

  # Reproducibility
  seed: 42

  # Mixed precision (saves ~40% VRAM, ~20% throughput gain)
  use_amp: true

  # torch.compile (PyTorch 2.0+, opt-in)
  # 10-40% throughput gain from op fusion. Disable if seeing compile errors.
  use_compile: false

  # DataLoader
  num_workers: 4
  pin_memory: true

  # Logging
  log_interval: 20   # unused by tqdm trainer, kept for reference
  eval_interval: 1   # validate every N epochs
  save_interval: 10  # checkpoint every N epochs

# Model Configuration
model:
  version: "v2"      # "v2" (LAST-v2) or "e" (LAST-E efficient)
  variant: "small"   # Options: small, base, large
  num_classes: 60    # 60 or 120 (Overridden by CLI if needed)

# Data Configuration
data:
  dataset:
    data_type: "mib" # "skeleton" (raw) or "mib" (multi-stream: joint/velocity/bone)




# Output
output:
  runs_root: "/kaggle/working/LAST-runs"  # Each run: run-YYYY-MM-DD_HH-MM-SS
