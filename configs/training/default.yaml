# LAST Training Configuration -- Default
# All hyperparameters for the training pipeline
#
# Batch size guidance (with AMP + adaptive graph):
#   Kaggle T4  (16GB): batch_size: 8,  accum: 4  → effective 32
#   GCP P100   (16GB): batch_size: 16, accum: 1  → effective 16
#   Lambda A10 (23GB, LAST-v2 base):   batch_size: 64  (3 streams × 9.2M params)
#   Lambda A10 (23GB, LAST-E any):     batch_size: 128 (tiny model, ample headroom)

training:
  # Optimizer (SGD + Nesterov is SOTA for GCNs like CTR-GCN)
  optimizer: "sgd"
  lr: 0.1          # SOTA skeleton GCN papers (CTR-GCN, MS-G3D, InfoGCN) use lr=0.1 with SGD
  momentum: 0.9    # Standard momentum
  nesterov: true   # Essential for convergence
  weight_decay: 0.0004  # Applied only to conv/linear weights (bias/BN/alpha/A_learned excluded)

  # LR Schedule: LinearLR warmup → MultiStepLR via SequentialLR
  # multistep_warmup is de facto standard for supervised skeleton GCN (vs cosine for SSL).
  # milestones are GLOBAL epoch numbers; trainer subtracts warmup_epochs internally.
  # Fully serialized into checkpoint — safe to resume at any epoch.
  scheduler: "multistep_warmup"
  warmup_epochs: 5       # Short warmup; adaptive graph stabilizes quickly at lr=0.1
  warmup_start_lr: 0.01  # Start at lr/10
  milestones: [50, 65]   # 10x LR drops at these global epochs (scaled from [60,90]/90)
  gamma: 0.1             # 10x drop factor
  min_lr: 0.0001         # Kept for cosine_warmup fallback (unused by multistep_warmup)

  # Training loop
  epochs: 90        # Papers use 90-120; MultiStep needs room after last milestone (65)
  batch_size: 64    # Lambda A10 23GB (LAST-v2 base); use 8-16 for Kaggle T4/P100
  gradient_clip: 2.0  # Relaxed from 1.0 — allows full gradient flow at lr=0.1

  # Gradient accumulation
  # effective_batch = batch_size × gradient_accumulation_steps
  # GCP P100:  batch_size: 16, gradient_accumulation_steps: 1  → effective 16
  # Kaggle T4: batch_size: 8, gradient_accumulation_steps: 4  → effective 32
  gradient_accumulation_steps: 1

  # Loss
  label_smoothing: 0.1

  # Input
  input_frames: 64  # temporal crop during training

  # Reproducibility
  seed: 42

  # Mixed precision (saves ~40% VRAM, ~20% throughput gain)
  use_amp: true

  # torch.compile (PyTorch 2.0+, opt-in)
  # 10-40% throughput gain from op fusion. Disable if seeing compile errors.
  use_compile: false

  # DataLoader
  num_workers: 4
  pin_memory: true

  # Logging
  log_interval: 20   # unused by tqdm trainer, kept for reference
  eval_interval: 1   # validate every N epochs
  save_interval: 10  # checkpoint every N epochs

# Model Configuration
model:
  version: "v2"      # "v2" (LAST-v2) or "e" (LAST-E efficient)
  variant: "small"   # Options: small, base, large
  num_classes: 60    # 60 or 120 (Overridden by CLI if needed)

# Data Configuration
data:
  dataset:
    data_type: "mib" # "skeleton" (raw) or "mib" (multi-stream: joint/velocity/bone)




# Output
output:
  runs_root: "/kaggle/working/LAST-runs"  # Each run: run-YYYY-MM-DD_HH-MM-SS
