# LAST Training Configuration -- Default
# All hyperparameters for the training pipeline
#
# Batch size guidance for Kaggle T4 (16GB VRAM) with AMP + adaptive graph:
#   small:  batch_size: 8, gradient_accumulation_steps: 3  → effective 24
#   base:   batch_size: 8, gradient_accumulation_steps: 4  → effective 32
#   large:  batch_size: 8, gradient_accumulation_steps: 4  → effective 32

training:
  # Optimizer (SGD + Nesterov is SOTA for GCNs like CTR-GCN)
  optimizer: "sgd"
  lr: 0.1          # Standard base LR for SGD
  momentum: 0.9    # Standard momentum
  nesterov: true   # Essential for convergence
  weight_decay: 0.0004  # Applied only to conv/linear weights (bias/BN/alpha/A_learned excluded)

  # LR Schedule: LinearLR warmup → CosineAnnealingLR via SequentialLR
  # Fully serialized into checkpoint — safe to resume at any epoch
  scheduler: "cosine_warmup"
  warmup_epochs: 5       # Linear ramp: warmup_start_lr → lr over 5 epochs
  warmup_start_lr: 0.01  # Start at lr/10
  min_lr: 0.0001         # Cosine floor

  # Training loop
  epochs: 70        # SOTA usually 65-80 epochs
  batch_size: 8     # Safe for Kaggle T4 16GB with adaptive graph
  gradient_clip: 5.0

  # Gradient accumulation
  # effective_batch = batch_size × gradient_accumulation_steps
  # Default: 8 × 4 = 32 effective batch for Base/Large on Kaggle
  gradient_accumulation_steps: 4

  # Loss
  label_smoothing: 0.1

  # Input
  input_frames: 64  # temporal crop during training

  # Reproducibility
  seed: 42

  # Mixed precision (saves ~40% VRAM, ~20% throughput gain)
  use_amp: true

  # torch.compile (PyTorch 2.0+, opt-in)
  # 10-40% throughput gain from op fusion. Disable if seeing compile errors.
  use_compile: false

  # DataLoader
  num_workers: 4
  pin_memory: true

  # Logging
  log_interval: 20   # unused by tqdm trainer, kept for reference
  eval_interval: 1   # validate every N epochs
  save_interval: 10  # checkpoint every N epochs

# Model Configuration
model:
  version: "v2"      # "v1" (Original AGCN) or "v2" (LAST-v2 Hybrid)
  variant: "small"   # Options: small, base, large
  num_classes: 60    # 60 or 120 (Overridden by CLI if needed)

# Data Configuration
data:
  dataset:
    data_type: "mib" # "skeleton" (raw), "npy" (v1 single stream), "mib" (v2 multi-stream)




# Output
output:
  runs_root: "/kaggle/working/LAST-runs"  # Each run: run-YYYY-MM-DD_HH-MM-SS
