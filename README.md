# LAST: Lightweight Adaptive-Shift Transformer

Skeleton-based action recognition with efficient temporal modeling.

## ğŸ“ Project Structure

```
LAST/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data/                     # Data loading âœ…
â”‚   â”‚   â”œâ”€â”€ skeleton_loader.py    # .skeleton file parser
â”‚   â”‚   â””â”€â”€ dataset.py            # PyTorch Dataset
â”‚   â”œâ”€â”€ models/                   # Model architectures (TODO)
â”‚   â”œâ”€â”€ training/                 # Training logic (TODO)
â”‚   â””â”€â”€ utils/                    # Utilities âœ…
â”‚       â”œâ”€â”€ config.py             # Config loader âœ…
â”‚       â””â”€â”€ visualization.py      # Skeleton visualization
â”œâ”€â”€ configs/                      # Configuration files âœ…
â”‚   â”œâ”€â”€ environment/              # Environment configs
â”‚   â”‚   â”œâ”€â”€ local.yaml            # Local development âœ…
â”‚   â”‚   â””â”€â”€ kaggle.yaml           # Kaggle execution âœ…
â”‚   â””â”€â”€ data/                     # Dataset configs
â”‚       â””â”€â”€ ntu120.yaml           # NTU RGB+D 120 âœ…
â”œâ”€â”€ scripts/                      # Execution scripts
â”‚   â”œâ”€â”€ load_data.py              # Config-driven data loading âœ…
â”‚   â”œâ”€â”€ test_dataloader.py        # Test/validation âœ…
â”‚   â””â”€â”€ quick_test.py             # Quick validation âœ…
â”œâ”€â”€ tests/                        # Unit tests (TODO)
â”œâ”€â”€ environment_setup.txt         # Python dependencies âœ…
â””â”€â”€ activate_ai.bat               # Environment activation âœ…
```

## ğŸš€ Quick Start - Config-Driven Data Loading

### 1. Install Dependencies

```bash
# Activate environment
activate_ai.bat

# Install if not done yet
pip install -r environment_setup.txt
```

### 2. Configuration Files

The project uses **YAML configs** for all parameters:

**Environment configs** (`configs/environment/`):
- `local.yaml` - Local Windows development
- `kaggle.yaml` - Kaggle notebook execution

**Data configs** (`configs/data/`):
- `ntu120.yaml` - NTU RGB+D 120 dataset parameters

### 3. Load Data (Production Way)

```bash
# Local environment (auto-detected)
python scripts/load_data.py --split train

# Explicitly specify environment
python scripts/load_data.py --env local --split train

# Kaggle environment
python scripts/load_data.py --env kaggle --split val
```

### 4. Test Data Loader (Validation Only)

For testing/debugging only (not production):
```bash
python scripts/quick_test.py
```

## ğŸ“Š Data Format

**Input:** `.skeleton` files from NTU RGB+D 120
- 103 frames (example)
- 25 joints per frame
- 3D coordinates (x, y, z) in meters

**Output:** PyTorch tensors
- Shape: `(C, T, V, M)` = `(3, 300, 25, 2)`
- C = coordinates, T = frames, V = joints, M = max bodies

## ğŸ¯ Next Steps

1. âœ… Data loading - **COMPLETED**
2. â³ Data preprocessing (.skeleton â†’ .npy)
3. â³ Model implementation
4. â³ Training pipeline
5. â³ Evaluation

## ğŸ“ Current Status

**Phase 1: Data Pipeline** - âœ… Core Implementation Done
- Skeleton file parser
- PyTorch Dataset with cross-subject/cross-setup splits
- 3D visualization utilities
- Test script for validation

Ready to test with your NTU RGB+D data!
