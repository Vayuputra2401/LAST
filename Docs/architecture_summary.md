# LAST Code Architecture - Complete Summary

This document provides an overview of the complete LAST (Lightweight Adaptive-Shift Transformer) code architecture with multi-environment support.

---

## ğŸ“ Complete File Structure

```
LAST/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ environment/              # Environment-specific configs (NEW)
â”‚   â”‚   â”œâ”€â”€ local.yaml           # Local machine settings
â”‚   â”‚   â”œâ”€â”€ gcp.yaml             # GCP instance settings
â”‚   â”‚   â”œâ”€â”€ gcp_instance.yaml   # GCP VM specifications
â”‚   â”‚   â””â”€â”€ kaggle.yaml          # Kaggle environment settings
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ ntu120_xsub.yaml
â”‚   â”‚   â”œâ”€â”€ ntu120_xset.yaml
â”‚   â”‚   â””â”€â”€ kinetics_skeleton.yaml
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ last_base.yaml
â”‚   â”‚   â”œâ”€â”€ last_large.yaml
â”‚   â”‚   â””â”€â”€ last_tiny.yaml
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ baseline.yaml
â”‚   â”‚   â”œâ”€â”€ distillation.yaml
â”‚   â”‚   â””â”€â”€ ablation_*.yaml
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ evaluation.yaml
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ inference.yaml
â”‚   â””â”€â”€ export/
â”‚       â”œâ”€â”€ onnx.yaml
â”‚       â””â”€â”€ quantization.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ transforms.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â””â”€â”€ skeleton_loader.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ last.py
â”‚   â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”‚   â”œâ”€â”€ agcn.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tsm.py
â”‚   â”‚   â”‚   â””â”€â”€ linear_attn.py
â”‚   â”‚   â”œâ”€â”€ teacher.py
â”‚   â”‚   â””â”€â”€ registry.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ optimizer.py
â”‚   â”‚   â””â”€â”€ scheduler.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ predictor.py
â”‚   â”‚   â””â”€â”€ video_processor.py
â”‚   â”œâ”€â”€ export/
â”‚   â”‚   â”œâ”€â”€ onnx_exporter.py
â”‚   â”‚   â””â”€â”€ quantizer.py
â”‚   â”œâ”€â”€ cloud/                    # Multi-cloud support (NEW)
â”‚   â”‚   â”œâ”€â”€ environment.py       # Auto-detect local/GCP/Kaggle
â”‚   â”‚   â”œâ”€â”€ gcs_manager.py       # Google Cloud Storage ops
â”‚   â”‚   â””â”€â”€ instance_manager.py  # GCP instance lifecycle
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ checkpoint.py
â”‚       â”œâ”€â”€ visualization.py
â”‚       â””â”€â”€ seed.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Main training with CLI overrides
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ export_model.py
â”‚   â”œâ”€â”€ precompute_teacher.py
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â””â”€â”€ gcp/                     # GCP-specific scripts (NEW)
â”‚       â”œâ”€â”€ upload_to_gcp.py
â”‚       â”œâ”€â”€ download_results.py
â”‚       â”œâ”€â”€ setup_environment.sh
â”‚       â””â”€â”€ run_training.sh
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â””â”€â”€ test_inference.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸŒ Multi-Environment Support Matrix

| Feature | Local | GCP | Kaggle |
|---------|-------|-----|--------|
| **Environment Detection** | Default | Metadata API | `/kaggle` path |
| **Config File** | `local.yaml` | `gcp.yaml` | `kaggle.yaml` |
| **Data Storage** | Local disk | GCS + Local SSD | Kaggle Datasets |
| **GPU** | Optional | T4 ($0.35/hr) | P100/T4 (Free) |
| **RAM** | Variable | 30 GB | 13 GB |
| **CPU Cores** | Variable | 8 vCPUs | 2 vCPUs |
| **Max Runtime** | Unlimited | Until stopped | 9-12 hours |
| **Disk Space** | Unlimited | 100GB + 375GB SSD | 73GB (20GB usable) |
| **Auto-Sync** | No | GCS bucket | Kaggle Datasets |
| **Auto-Shutdown** | No | Yes (config) | Session timeout |
| **Cost** | Free (hardware) | ~$0.83/hr | Free |
| **Best For** | Development | Full training | Quick experiments |

---

## ğŸš€ Quick Start Commands

### Auto-Detect Environment
```bash
# Automatically detects and uses appropriate config
python scripts/train.py
```

### Specify Environment
```bash
# Local machine
python scripts/train.py --env local

# GCP instance
python scripts/train.py --env gcp

# Kaggle kernel
python scripts/train.py --env kaggle
```

### With Overrides
```bash
# Override batch size and learning rate
python scripts/train.py --env kaggle --batch_size 16 --lr 0.0005

# Override data path
python scripts/train.py --data_path /custom/path --checkpoint_dir ./ckpts

# Quick debug run
python scripts/train.py --debug --epochs 2 --batch_size 8

# Dry run (validate config only)
python scripts/train.py --dry_run --env gcp
```

---

## ğŸ“‹ Typical Workflows

### 1. Local Development & Testing
```bash
# On your laptop (Windows)
cd C:\Users\pathi\OneDrive\Desktop\LAST

# Quick test with small batch
python scripts/train.py --env local --batch_size 8 --epochs 2 --debug

# Full baseline training (CPU, slower)
python scripts/train.py --env local --train_config configs/train/baseline.yaml
```

### 2. Kaggle Experimentation
```python
# In Kaggle notebook

# Cell 1: Setup
!pip install -q -r requirements.txt

# Cell 2: Quick experiment
!python scripts/train.py \
    --env kaggle \
    --model_config configs/model/last_tiny.yaml \
    --epochs 10 \
    --batch_size 24

# Cell 3: Save results
!zip -r results.zip /kaggle/working/checkpoints /kaggle/working/logs
```

### 3. GCP Full Training
```bash
# Step 1: Upload code from local machine
python scripts/gcp/upload_to_gcp.py

# Step 2: SSH to GCP instance
gcloud compute ssh last-training-gpu --zone=asia-east1-c

# Step 3: Setup environment (on GCP)
cd ~/last
bash scripts/gcp/setup_environment.sh

# Step 4: Run training (on GCP)
bash scripts/gcp/run_training.sh
# OR with overrides:
python scripts/train.py --env gcp --epochs 150 --batch_size 64

# Step 5: Download results (back on local)
python scripts/gcp/download_results.py --experiment last_baseline_001
```

---

## ğŸ¯ Configuration Override Priority

**Highest â†’ Lowest Priority:**

1. **CLI Arguments** (`--batch_size 32`)
2. **Training Config** (`configs/train/baseline.yaml`)
3. **Environment Config** (`configs/environment/kaggle.yaml`)
4. **Code Defaults**

**Example:**
```bash
# Environment says batch_size=32
# Training config says batch_size=64
# CLI says --batch_size 16

python scripts/train.py --env kaggle --batch_size 16
# Final batch_size = 16 (CLI wins!)
```

---

## ğŸ”§ Key Design Principles

### 1. YAGNI (You Aren't Gonna Need It)
- No over-engineering
- Features added only when needed
- Clean, minimal abstractions

### 2. KISS (Keep It Simple, Stupid)
- Single responsibility per class
- Explicit function signatures
- Shallow inheritance (max 2 levels)

### 3. DRY (Don't Repeat Yourself)
- Centralized config loading
- Shared preprocessing logic
- Reusable metric computation

### 4. Config-Driven Development
- **Zero code changes** between environments
- All settings in YAML files
- CLI overrides for flexibility

### 5. Environment Agnostic
- Automatic environment detection
- Path resolution (Windows/Linux/Kaggle)
- Conditional cloud integrations

---

## ğŸ“¦ Main Components

### Data Pipeline
- `SkeletonDataset`: Main dataset class
- `SkeletonFileParser`: Parse NTU .skeleton files
- `SkeletonTransform`: Composable augmentations
- Auto-detect `.skeleton` or `.npy` format

### Model Architecture
- `LAST`: Main model (composite pattern)
- `LASTBlock`: A-GCN + TSM + Linear Attention
- `AdaptiveGCN`: Learnable graph convolution
- `TemporalShiftModule`: Zero-param temporal modeling
- `LinearAttention`: O(T) efficient attention
- `TeacherModel`: VideoMAE V2 wrapper

### Training System
- `Trainer`: Orchestrates full training loop
- `LossFunction`: Classification + Distillation
- `OptimizerFactory`: Creates optimizers from config
- `SchedulerFactory`: Creates LR schedulers from config

### Cloud Integration
- `EnvironmentDetector`: Auto-detect local/GCP/Kaggle
- `GCSManager`: Upload/download to Google Cloud Storage
- `InstanceManager`: GCP lifecycle (start/stop/delete)

### Evaluation & Export
- `Evaluator`: Compute metrics (accuracy, FLOPs, latency)
- `MetricCalculator`: Top-k accuracy, confusion matrix
- `ONNXExporter`: Export to ONNX format
- `ModelQuantizer`: INT8 quantization

---

## ğŸ“š Documentation Files

1. **`code_architecture.md`** - Core architecture (original)
   - Project structure
   - Class/function signatures
   - Design patterns
   - Clean code principles

2. **`gcp_support_addition.md`** - GCP integration
   - GCP environment config
   - GCS storage integration
   - Instance management
   - Upload/download scripts

3. **`kaggle_cli_addition.md`** - Kaggle & CLI
   - Kaggle environment config
   - CLI argument parsing
   - Override system
   - Multi-environment workflows

4. **`architecture_summary.md`** - This file
   - Complete overview
   - Quick reference
   - Common workflows

---

## ğŸ“ Next Steps

### Phase 1: Setup (Week 1)
1. Create all config files (`configs/environment/*.yaml`)
2. Implement `EnvironmentDetector` class
3. Test auto-detection on local/GCP/Kaggle
4. Implement `ConfigLoader` with override support

### Phase 2: Data Pipeline (Week 2)
1. Implement `SkeletonFileParser`
2. Build `SkeletonDataset`
3. Create transforms (augmentations)
4. Test on small NTU subset

### Phase 3: Model (Week 3-4)
1. Implement `AdaptiveGCN`
2. Implement `TemporalShiftModule`
3. Implement `LinearAttention`
4. Assemble `LASTBlock`
5. Build full `LAST` model

### Phase 4: Training (Week 5)
1. Implement `Trainer` class
2. Build `LossFunction` (CE + KD)
3. Test Phase 1 training (baseline, skeleton-only)
4. Verify convergence on small dataset

### Phase 5: Cloud Integration (Week 6)
1. Implement `GCSManager`
2. Implement `InstanceManager`
3. Test GCP upload/training/download workflow
4. Test Kaggle kernel execution

### Phase 6: Evaluation & Export (Week 7)
1. Implement `Evaluator`
2. Build metric computation
3. Implement ONNX export
4. Add quantization support

### Phase 7: Full Training (Week 8+)
1. Download NTU RGB videos
2. Pre-compute teacher logits
3. Phase 2 training (distillation)
4. Ablation studies
5. Paper writing

---

## âœ… Key Benefits

âœ… **Multi-Environment**: Runs on local, GCP, Kaggle without code changes  
âœ… **Config-Driven**: Everything controlled via YAML files  
âœ… **CLI Overrides**: Flexible command-line argument system  
âœ… **Auto-Detection**: Automatically detects execution environment  
âœ… **Cloud Integration**: Automatic GCS sync, auto-shutdown  
âœ… **Modular**: Clean separation of concerns (SOLID principles)  
âœ… **Extensible**: Easy to add new models, losses, datasets  
âœ… **Debuggable**: Clear interfaces, comprehensive logging  
âœ… **Cost-Effective**: Free Kaggle for experiments, paid GCP for full runs  

---

## ğŸ”¥ Most Common Commands

```bash
# Development (local)
python scripts/train.py --debug --epochs 1

# Testing (Kaggle)
python scripts/train.py --env kaggle --model_config configs/model/last_tiny.yaml

# Production (GCP)
python scripts/train.py --env gcp --train_config configs/train/distillation.yaml

# Hyperparameter sweep
python scripts/train.py --lr 0.001 --experiment_name lr_001
python scripts/train.py --lr 0.0005 --experiment_name lr_0005

# Resume training
python scripts/train.py --resume checkpoints/epoch_50.pth

# Validation only
python scripts/eval.py --checkpoint checkpoints/best.pth

# Export model
python scripts/export_model.py --checkpoint checkpoints/best.pth --format onnx
```

---

**Architecture Designed For:** Maximum flexibility, minimal code changes, seamless multi-environment execution!
