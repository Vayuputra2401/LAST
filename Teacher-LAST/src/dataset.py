"""
Teacher-LAST: NTU RGB+D 60 Video Dataset
==========================================
Video dataset loader for fine-tuning VideoMAE-2 on NTU RGB+D 60.

Frame Sampling Strategy (from official VideoMAE kinetics.py):

  Training & Validation (SAME temporal sampling — official behavior):
    - Long videos (≥ 64 frames): Random temporal crop
        → Pick random 64-frame window → sample every 4th frame → 16 frames
    - Short videos (< 64 frames): Last-frame clamping
        → Uniform sample available frames → repeat last frame to fill 16
    - Difference is ONLY in spatial transforms:
        Train: RandomCrop(224) + RandomHorizontalFlip
        Val:   CenterCrop(224) — no randomness spatially

  Testing:
    - Load ALL frames at sample rate → 10 temporal clips × 3 spatial crops
    - 30 predictions per video, averaged for final answer
    - Fully deterministic

Data Format:
    CSV annotation files with format: <video_path> <label_id>
    Generated by scripts/prepare_annotations.py

Dependencies:
    - decord: High-performance video decoding
    - numpy: Frame index computation
    - torch: Dataset/DataLoader integration
"""

import os
import warnings
import numpy as np
import torch
from torch.utils.data import Dataset
from decord import VideoReader, cpu
from torchvision import transforms


# =============================================================================
# Spatial Transforms (from official VideoMAE)
# =============================================================================

def get_train_transform(input_size=224, mean=None, std=None):
    """
    Training spatial transforms.
    
    Following official VideoMAE: resize short side → random crop → normalize.
    No heavy augmentation — mixup/cutmix are applied at the batch level in trainer.
    
    Args:
        input_size: Spatial crop size (224 for ViT-L)
        mean: ImageNet channel means
        std: ImageNet channel stds
    
    Returns:
        Callable transform
    """
    if mean is None:
        mean = [0.485, 0.456, 0.406]  # ImageNet means
    if std is None:
        std = [0.229, 0.224, 0.225]   # ImageNet stds

    return transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.RandomCrop(input_size),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std),
    ])


def get_val_transform(input_size=224, mean=None, std=None):
    """
    Validation/test spatial transforms.
    
    Deterministic: resize → center crop → normalize.
    
    Args:
        input_size: Spatial crop size
        mean: ImageNet channel means
        std: ImageNet channel stds
    
    Returns:
        Callable transform
    """
    if mean is None:
        mean = [0.485, 0.456, 0.406]
    if std is None:
        std = [0.229, 0.224, 0.225]

    return transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.CenterCrop(input_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std),
    ])


# =============================================================================
# NTU RGB+D 60 Video Dataset
# =============================================================================

class NTU60Dataset(Dataset):
    """
    NTU RGB+D 60 video classification dataset.
    
    Loads videos from CSV annotation files, samples frames according to the
    official VideoMAE strategy, and applies spatial transforms.
    
    Args:
        csv_path: Path to annotation CSV (format: <video_path> <label_id>)
        config: Configuration dictionary (data section)
        mode: One of 'train', 'validation', 'test'
        test_num_segment: Number of temporal clips for multi-clip testing
        test_num_crop: Number of spatial crops for multi-clip testing
    """

    def __init__(self, csv_path, config, mode='train',
                 test_num_segment=5, test_num_crop=3):
        self.csv_path = csv_path
        self.mode = mode

        # Video sampling parameters
        self.clip_len = config.get('num_frames', 16)
        self.frame_sample_rate = config.get('sampling_rate', 4)
        self.input_size = config.get('input_size', 224)
        self.short_side_size = config.get('short_side_size', 224)

        # Multi-clip testing parameters
        self.test_num_segment = test_num_segment
        self.test_num_crop = test_num_crop

        # Load annotation file
        # Format: <absolute_video_path> <label_id>
        self.samples = []
        self.labels = []
        with open(csv_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.rsplit(' ', 1)  # Split from right to handle paths with spaces
                if len(parts) != 2:
                    warnings.warn(f"Skipping malformed line: {line}")
                    continue
                video_path, label = parts
                self.samples.append(video_path)
                self.labels.append(int(label))

        print(f"[Dataset] Loaded {len(self.samples)} videos from {csv_path} (mode={mode})")

        # Spatial transforms
        if mode == 'train':
            self.transform = get_train_transform(self.input_size)
        else:
            self.transform = get_val_transform(self.input_size)

        # For multi-clip testing, expand the dataset
        if mode == 'test':
            self.test_samples = []
            self.test_labels = []
            self.test_seg = []
            for ck in range(self.test_num_segment):
                for cp in range(self.test_num_crop):
                    for idx in range(len(self.samples)):
                        self.test_samples.append(self.samples[idx])
                        self.test_labels.append(self.labels[idx])
                        self.test_seg.append((ck, cp))

    def __len__(self):
        if self.mode == 'test':
            return len(self.test_samples)
        return len(self.samples)

    def __getitem__(self, index):
        if self.mode == 'train':
            return self._get_train_item(index)
        elif self.mode == 'validation':
            return self._get_val_item(index)
        elif self.mode == 'test':
            return self._get_test_item(index)
        else:
            raise ValueError(f"Unknown mode: {self.mode}")

    # -------------------------------------------------------------------------
    # Training: Random temporal crop (long) / Last-frame clamp (short)
    # -------------------------------------------------------------------------

    def _get_train_item(self, index):
        """
        Training data loading with random temporal crop.
        
        If video fails to load, randomly picks another video (avoids crash).
        """
        video_path = self.samples[index]
        label = self.labels[index]

        buffer = self._load_video(video_path, mode='train')

        # Retry on failed videos
        while buffer is None:
            warnings.warn(f"Video not loaded: {video_path}")
            index = np.random.randint(len(self.samples))
            video_path = self.samples[index]
            label = self.labels[index]
            buffer = self._load_video(video_path, mode='train')

        # Apply spatial transforms per frame → stack to (C, T, H, W)
        frames = self._apply_transforms(buffer)

        return frames, label, index

    # -------------------------------------------------------------------------
    # Validation: Same random temporal crop as training (official behavior)
    #             Only spatial transforms differ (CenterCrop vs RandomCrop)
    # -------------------------------------------------------------------------

    def _get_val_item(self, index):
        """
        Validation data loading.
        
        Official VideoMAE behavior: SAME temporal sampling as training
        (random temporal crop for long videos, last-frame clamp for short).
        Only the spatial transform is different (CenterCrop instead of RandomCrop).
        
        This means validation accuracy may fluctuate ±1-2% between epochs
        due to temporal sampling noise. The official team relies on test-time
        multi-clip evaluation for the final deterministic metric.
        """
        video_path = self.samples[index]
        label = self.labels[index]

        # Uses same _load_video → same _sample_frame_indices as training
        buffer = self._load_video(video_path, mode='validation')

        # Retry on failed videos
        while buffer is None:
            warnings.warn(f"Video not loaded: {video_path}")
            index = np.random.randint(len(self.samples))
            video_path = self.samples[index]
            label = self.labels[index]
            buffer = self._load_video(video_path, mode='validation')

        # Spatial transforms: CenterCrop (set in __init__ based on mode)
        frames = self._apply_transforms(buffer)

        return frames, label, index

    # -------------------------------------------------------------------------
    # Testing: Multi-clip (5 temporal × 3 spatial)
    # -------------------------------------------------------------------------

    def _get_test_item(self, index):
        """
        Test data loading with multi-clip evaluation.
        
        Each (chunk_nb, split_nb) pair defines one of 15 clips:
        - chunk_nb: Temporal position (0-4, evenly spaced)
        - split_nb: Spatial crop (0=left/top, 1=center, 2=right/bottom)
        """
        video_path = self.test_samples[index]
        label = self.test_labels[index]
        chunk_nb, split_nb = self.test_seg[index]

        buffer = self._load_video(video_path, mode='test')

        while buffer is None:
            warnings.warn(f"Video not loaded: {video_path}")
            index = np.random.randint(len(self.test_samples))
            video_path = self.test_samples[index]
            label = self.test_labels[index]
            chunk_nb, split_nb = self.test_seg[index]
            buffer = self._load_video(video_path, mode='test')

        # For test mode, temporal clipping is handled here
        buffer = self._temporal_clip(buffer, chunk_nb)

        # Spatial crop based on split_nb
        frames = self._apply_test_transforms(buffer, split_nb)

        video_name = os.path.splitext(os.path.basename(video_path))[0]
        return frames, label, video_name, chunk_nb, split_nb

    # -------------------------------------------------------------------------
    # Video Loading (decord)
    # -------------------------------------------------------------------------

    def _load_video(self, video_path, mode='train'):
        """
        Load video frames using decord VideoReader.
        
        Args:
            video_path: Absolute path to video file
            mode: 'train', 'validation', or 'test'
        
        Returns:
            np.ndarray: Sampled frames (T, H, W, C) or None if failed
        """
        if not os.path.exists(video_path):
            return None

        # Skip tiny files (likely corrupt)
        if os.path.getsize(video_path) < 1024:
            print(f"SKIP (too small): {video_path}")
            return None

        try:
            vr = VideoReader(video_path, num_threads=1, ctx=cpu(0))
        except Exception:
            print(f"SKIP (decord error): {video_path}")
            return None

        total_frames = len(vr)
        if total_frames < 1:
            return None

        # Get frame indices based on mode
        if mode == 'test':
            # Test mode: load ALL frames at sample rate (temporal clipping later)
            indices = list(range(0, total_frames, self.frame_sample_rate))
            # Last-frame clamp if too short
            while len(indices) < self.clip_len:
                indices.append(indices[-1])
        else:
            indices = self._sample_frame_indices(total_frames, mode=mode)

        # Fetch frames from video
        vr.seek(0)
        buffer = vr.get_batch(indices).asnumpy()  # (T, H, W, C)

        return buffer

    # -------------------------------------------------------------------------
    # Frame Sampling Logic
    # -------------------------------------------------------------------------

    def _sample_frame_indices(self, total_frames, mode='train'):
        """
        Sample frame indices following the official VideoMAE strategy.
        
        SAME logic for BOTH training and validation (official behavior).
        The mode parameter is accepted for API compatibility but does not
        change the sampling behavior — this matches the official code where
        both train and val call the same loadvideo_decord() function.
        
        Long videos (≥ 64 frames): Random temporal crop
            → Pick random 64-frame window → sample 16 frames evenly
            → Each call sees a different slice of the action
        
        Short videos (< 64 frames): Last-frame clamping
            → Uniform sample available frames → repeat last frame
            → The video "freezes" on the final pose
        
        Args:
            total_frames: Total number of frames in the video
            mode: 'train' or 'validation' (same behavior for both)
        
        Returns:
            np.ndarray: Frame indices of shape (clip_len,)
        """
        converted_len = self.clip_len * self.frame_sample_rate  # 16 * 4 = 64

        if total_frames <= converted_len:
            # -----------------------------------------------------------------
            # SHORT VIDEO (< 64 frames): Last-frame clamping
            # -----------------------------------------------------------------
            # Sample as many real frames as possible at the sampling rate,
            # then repeat the last frame to fill remaining slots.
            #
            # Example: 30-frame video, need 16 frames
            #   available = 30 // 4 = 7
            #   linspace(0, 30, 7) → [0, 5, 10, 15, 20, 25, 29]
            #   Pad 9 more with frame 29 → [..., 29, 29, 29, 29, 29, 29, 29, 29, 29]
            available = max(total_frames // self.frame_sample_rate, 1)
            index = np.linspace(0, total_frames, num=available)
            index = np.concatenate((
                index,
                np.ones(self.clip_len - available) * (total_frames - 1)
            ))
            index = np.clip(index, 0, total_frames - 1).astype(np.int64)
        else:
            # -----------------------------------------------------------------
            # LONG VIDEO (≥ 64 frames): Random temporal crop
            # -----------------------------------------------------------------
            # Pick a random 64-frame window and sample every 4th frame.
            # Each epoch sees a different slice of the action.
            #
            # Example: 150-frame video
            #   end_idx = random(64, 150) → e.g., 100
            #   start_idx = 100 - 64 = 36
            #   linspace(36, 100, 16) → [36, 40, 44, 48, ..., 96]
            end_idx = np.random.randint(converted_len, total_frames)
            start_idx = end_idx - converted_len
            index = np.linspace(start_idx, end_idx, num=self.clip_len)
            index = np.clip(index, start_idx, end_idx - 1).astype(np.int64)

        return index

    # -------------------------------------------------------------------------
    # Temporal Clipping for Multi-Clip Testing
    # -------------------------------------------------------------------------

    def _temporal_clip(self, buffer, chunk_nb):
        """
        Extract a temporal clip from the full-video buffer for testing.
        
        The video is divided into test_num_segment evenly-spaced clips.
        chunk_nb selects which clip to use (0 to test_num_segment-1).
        
        Args:
            buffer: Full video buffer (T', H, W, C) — all frames at sample rate
            chunk_nb: Temporal clip index
        
        Returns:
            np.ndarray: Clipped frames (clip_len, H, W, C)
        """
        total_available = buffer.shape[0]

        if total_available <= self.clip_len:
            # Video too short even after sampling — use what we have
            return buffer[:self.clip_len]

        # Evenly space clips across the temporal dimension
        temporal_step = max(
            1.0 * (total_available - self.clip_len) / (self.test_num_segment - 1),
            0
        )
        temporal_start = int(chunk_nb * temporal_step)

        return buffer[temporal_start:temporal_start + self.clip_len]

    # -------------------------------------------------------------------------
    # Transform Application
    # -------------------------------------------------------------------------

    def _apply_transforms(self, buffer):
        """
        Apply spatial transforms to each frame and stack into model input.
        
        Args:
            buffer: Raw frames (T, H, W, C) as uint8 numpy array
        
        Returns:
            torch.Tensor: Transformed frames (C, T, H, W) as float32
                          VideoMAE expects channel-first format
        """
        frames = []
        for i in range(min(buffer.shape[0], self.clip_len)):
            frame = buffer[i]  # (H, W, C) uint8
            frame = self.transform(frame)  # (C, H, W) float32
            frames.append(frame)

        # Pad if we have fewer frames than clip_len (shouldn't happen, but safe)
        while len(frames) < self.clip_len:
            frames.append(frames[-1])

        # Stack: list of (C, H, W) → (T, C, H, W) → permute to (C, T, H, W)
        frames = torch.stack(frames, dim=0)       # (T, C, H, W)
        frames = frames.permute(1, 0, 2, 3)       # (C, T, H, W)

        return frames

    def _apply_test_transforms(self, buffer, split_nb):
        """
        Apply test transforms with 3-crop spatial evaluation.
        
        split_nb determines the spatial crop position:
          0: Left/Top crop
          1: Center crop
          2: Right/Bottom crop
        
        Args:
            buffer: Frames (T, H, W, C)
            split_nb: Spatial crop index (0, 1, or 2)
        
        Returns:
            torch.Tensor: (C, T, H, W)
        """
        frames = []
        for i in range(min(buffer.shape[0], self.clip_len)):
            frame = buffer[i]  # (H, W, C)

            # Resize short side to input_size
            from PIL import Image
            pil_img = Image.fromarray(frame)
            h, w = pil_img.size[1], pil_img.size[0]
            
            # Resize keeping aspect ratio (short side = input_size)
            if w < h:
                new_w = self.input_size
                new_h = int(h * self.input_size / w)
            else:
                new_h = self.input_size
                new_w = int(w * self.input_size / h)
            pil_img = pil_img.resize((new_w, new_h), Image.BILINEAR)

            # Spatial crop based on split_nb
            if new_h > new_w:
                # Portrait: top/center/bottom crops
                spatial_step = max(0, (new_h - self.input_size) // 2)
                if split_nb == 0:
                    crop_y = 0
                elif split_nb == 1:
                    crop_y = spatial_step
                else:
                    crop_y = new_h - self.input_size
                pil_img = pil_img.crop((0, crop_y, new_w, crop_y + self.input_size))
            else:
                # Landscape: left/center/right crops
                spatial_step = max(0, (new_w - self.input_size) // 2)
                if split_nb == 0:
                    crop_x = 0
                elif split_nb == 1:
                    crop_x = spatial_step
                else:
                    crop_x = new_w - self.input_size
                pil_img = pil_img.crop((crop_x, 0, crop_x + self.input_size, self.input_size))

            # Resize to exact input_size (in case of rounding)
            pil_img = pil_img.resize((self.input_size, self.input_size), Image.BILINEAR)

            # To tensor + normalize
            frame_tensor = transforms.ToTensor()(pil_img)
            frame_tensor = transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )(frame_tensor)
            frames.append(frame_tensor)

        while len(frames) < self.clip_len:
            frames.append(frames[-1])

        frames = torch.stack(frames, dim=0)   # (T, C, H, W)
        frames = frames.permute(1, 0, 2, 3)  # (C, T, H, W)

        return frames


# =============================================================================
# DataLoader Builder
# =============================================================================

def build_dataloader(config, mode='train'):
    """
    Build a DataLoader for the specified mode.
    
    Args:
        config: Full configuration dictionary
        mode: 'train' or 'validation'
    
    Returns:
        tuple: (DataLoader, Dataset)
    """
    data_config = config['data']
    training_config = config.get('training', {})

    # Select CSV path
    if mode == 'train':
        csv_path = data_config['train_csv']
    else:
        csv_path = data_config['val_csv']

    # Create dataset
    dataset = NTU60Dataset(
        csv_path=csv_path,
        config=data_config,
        mode=mode,
        test_num_segment=training_config.get('test_num_segment', 5),
        test_num_crop=training_config.get('test_num_crop', 3),
    )

    # Create DataLoader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=data_config.get('batch_size', config['training']['batch_size']),
        shuffle=(mode == 'train'),
        num_workers=data_config.get('num_workers', 10),
        pin_memory=data_config.get('pin_memory', True),
        drop_last=(mode == 'train'),
        persistent_workers=True if data_config.get('num_workers', 10) > 0 else False,
    )

    return dataloader, dataset
